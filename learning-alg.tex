\section{Algorithm for Learning of MMTs}
\label{sec:learning}

\todobj{The notation for concatenation between prefix and suffix may need
  checking in this section}

In this section, we present an algorithm for learning MMTs in then untimed
MAT of~\ref{sec:untimed-mat}, using the approximated Nerode equivalence
presented in Section~\ref{sec:approx}.
The learning algorithm follows the standard pattern for active automata learning
algorithms, such as $L^*$~\cite{Ang87}. It maintains
a set $U$ of lean behaviors, called {\em short prefixes}, which
represent states in the MMT to be constructed,
and an overapproximation of the Nerode equivalence,
parameterized by a set $V$ of input suffixes.
%% Each short prefix in $U$ is a feasible canonical untimed
%% behavior, which represents a state in the MMT to be constructed.
The learning algorithm iterates two phases: hypothesis construction and
hypothesis validation.
During hypothesis construction,
the approximation of the Nerode equivalence triggers the expansion of
$U$ and $V$ until two convergence conditions are satisfied that allow
a hypothesis automaton to be formed.
During hypothesis validation, the hypothesis automaton is submitted in an
equivalence query, and returned counterexamples are used to refine
the Nerode equivalence by expanding $V$.

\todobj{Say somewhere that we assume a fixed (un)timed language $S$.
  Make sure we make clear that it is the set of {\bf feasible} lean
behaviors of the SUL}

Let us introduce the two conditions for convergence of the construction phase.
For a feasible lean behavior $\beta$, let $\feasibleinputs{\beta}{S}$ be the
set of $i \in \extinputs$ such that
$\beta ; \simpleutlabel{i}{o} \in S$ for some $o$ (recall that the
last assignment of a lean behavior is always empty).
For $i \in \feasibleinputs{\beta}{S}$, let $\lambda(\beta,i)$ be the unique
output $o$ such that $\beta ; \simpleutlabel{i}{o} \in S$.
%% A set $V$ of generic untimed input words is {\em adequate} if it includes
%% $I \cup \set{\toevent{p}}$.
Let $U$ be a prefix-closed set of lean behaviors.
and let $V$ be an adequate set of input suffixes.
\begin{itemize}
\item
$U$ is {\em closed} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
%%   , which is either in $I$ or of form $\toevent{x_i}$ where $x_i$ is expirable after $\beta$ if
  there is a $\beta' \in U$ such that
  $\beta;\simpleutlabel{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$.
\item
$U$ is {\em timer-consistent} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
  %% $i$, which is either in $I$ or
  %% of form $\toevent{x_i}$ for $x_i$ expirable after $\beta$,
  we have
  $\apprgetmemorable{S}{\beta;\simpleutlabel{i}{\lambda(\beta,i)}}{V} \subseteq
  (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$.
\end{itemize}
Closedness ensures that each transition in the MMT to be constructed has a target state. 
Timer-consistency states that each timer which is needed after such
a transition (i.e., a timer set during $\beta;\simpleutlabel{i}{\lambda(\beta,i)}$)
is either a timer active after $\beta$ or is started by the last transition, thus
getting the name $x_{(|\beta|+1)}$.
Closedness and timer-consistency allow the construction of a hypothesis MMT.

\begin{definition}[Hypothesis automaton]
\label{def:hypo}
  Let $U$ be a non-empty prefix-closed set of lean behaviors,
  and $V$ an adequate set of input suffixes such that
  $U$ is closed and timer consistent wrt.\ $V$. Then the
{\em hypothesis automaton} $\hypoof UV$ is the MMT
$\hypoof UV = (I, O, Q, q_0, \vars, \delta, \lambda, \remap)$, where
\begin{itemize}
\item $Q = U$ and $q_0 = \emptyword$,
\item $\vars$ maps each location $\beta\in U$ to $\apprgetmemorable{S}{\beta}{V}$,
\item Let $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$. Then
  \begin{itemize}
    \item $\lambda(\beta,i)$ is
     the unique $o$ such that $\extend{\beta}{i}{o} \in S$. 
    \item $\delta(\beta,i)$ is is
        the unique $\beta' \in U$ such that there is an $f$ with
  $\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V}^f \beta'$.
  \item $\remap(\beta,i):\apprgetmemorable{S}{\beta'}{V} \mapsto (\apprgetmemorable{S}{\beta}{V} \cup \natplus)$ is defined as $\rho \circ f^{-1}$, where $\rho$ maps timer $x_{(|\beta|+1)}$ to
$\apprgetassignment{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}(x_{(|\beta|+1)})$.
  \end{itemize}
\item When $\beta \in U$ and $i$ is of form $\toevent{x_j}$ in
  $\apprgetmemorable{S}{\beta}{V} \setminus \feasibleinputs{\beta}{S}$, we let
  $\lambda(\beta,i) = \bot$, 
  $\delta(\beta,i) = \beta$, and let
  $\remap(\beta,i)$ be the identity mapping on $\apprgetmemorable{S}{\beta}{V}$.
\end{itemize}
\end{definition}
The last case in the construction constructs a transition that is not feasible,
but which must anyway be present since its input is a timeout event for
 a timer which can expire after performing additional transitions from $\beta$.

 During {\bf hypothesis construction}, membership queries are performed order
 to construct the sets of form $\apprsuffixbehs{S}{\beta}{V}$ and
 $\apprsuffixbehs{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V}$ for $\beta \in U$.
 Moreoever, the sets $U$ and $V$ are expanded if needed to construct
 a hypothesis automaton.

 More precisely,
membership queries are first performed
  for all untimed input words of form $\untimedinputword(\beta) \cdot i$
   for $\beta \in U$ and $i \in \extinputs$: this allows to determine
   $\feasibleinputs{\beta}{S}$ and $\lambda(\beta,i)$
   for $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
   Thereafter, membership queries are performed
  for all untimed input words of form $\untimedinputword(\beta) \cdot v$ and
  $\untimedinputword(\beta) \cdot i \cdot v$, where
  $\beta \in U$, $v \in V$, and
  $i \in \feasibleinputs{\beta}{S}$. Note that one need only consider
  $v$ in which inputs from $\toevents$ concern timers in
  $\set{x_1, \ldots, x_{|\beta|}}$ (or in  $\set{x_1, \ldots, x_{(|\beta|+1)}}$).
  This allows to construct the sets of form $\apprsuffixbehs{S}{\beta}{V}$ and
  $\apprsuffixbehs{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}$
  for $\beta \in U$.
  It then allows to compute the approximated Nerode equivalence $\equiv_{S,V}$ on
  the set of lean behaviors of form $\beta$ and
  $\extend{\beta}{i}{\lambda(\beta,i)}$ with
  $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
  We then check whether $U$ and $V$ meet the convergence criteria.
  \begin{itemize}
    \item
  Whenever the set $U$ is not closed wrt.\ $V$, then it is extended:
if there is some $\beta \in U$ and $i$ in $\feasibleinputs{\beta}{S}$
for which there is no $\beta' \in U$ such that
$\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$, 
then $\extend{\beta}{i}{\lambda(\beta,i)}$ is added to $U$,
triggering new membership queries.
\item
  Whenever the set $U$ is not timer-consistent wrt.\ $V$, then $V$ is extended:
for timer $x_j$ in
$\apprgetmemorable{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V} \setminus (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$, find a lean
suffix behavior of form $\extend{\gamma}{\toevent{x_j}}{o'}$ in
$\apprsuffixbehs{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}$, i.e.,
a suffix such that $x_j$ expires in the last transition. This expiration is
obviously missing from $\apprsuffixbehs{S}{\beta}{V}$. But, by
adding $i\cdot\untimedinputword(\gamma)\cdot\toevent{x_j}$ to $V$, the
timer will also appear in $\apprgetmemorable{S}{\beta}{V}$.
This extension of $V$ will trigger new membership queries.
\end{itemize}
When $U$ is closed and timer-consistent wrt.\ $V$, then
a hypothesis MMT $\hypoof UV$ is constructed and
validated by submitting it in an equivalence query.
If the query returns ``yes'', then
the learning is completed, and $\hypoof UV$ accepts $S$.
If the query returns a counterexample in the form of a lean behavior
$\alpha$, this is used to extend $V$ as follows.
We assume w.l.o.g.\ that no proper prefix of $\alpha$ is a counterexample. 
By the fact that $\alpha$ is a counterexample, we can write $\alpha$ as
$(\alpha';\simpleutlabel{i}{o}); \gamma$, where $\gamma$ is a lean suffix
behavior such that
$\beta\cdot \simpleutlabel{i}{o} \equiv_{S,V}^f \beta'$ but
$(\beta ; \simpleutlabel{i}{o}) ; \gamma) \in S \notiff \beta' ; f(\gamma) \in S$ for some $\beta,\beta' \in U$ such that 
$\beta;\simpleutlabel{i}{o} \equiv_{S,V}^f \beta'$ is used to construct
the transition triggered by $i$ from $\beta$ in $\hypoof UV$.
To se this, let $\alpha = \utlabel{i_1}{o_1}{\rho_1} \cdots \utlabel{i_n}{o_n}{\rho_n}$, and for $j = 0 , \ldots, n$, define
a lean behavior $\beta_j$ and a $\beta_j$-suffix by
\item $\beta_0 = \emptyword$, and
%%   , $f_0$ is arbitrary, and 
  $\gamma_0$ is obtained from $\alpha$ by renaming each timer $x_l$ to $y_l$
\item
  Let $\gamma_{j-1}$ be of form
  $\simpleutlabel{\hat{i_j}}{\hat{o_j}} \gamma_{j-1}'$,
  let $f_j$ be the mapping used to establish
  $\beta_{j-1};\simpleutlabel{\hat{i_j}}{\hat{o_j}} \equiv_{S,V}^{f_j} \beta_{j}$
  in the construction of $\hypoof UV$, and let
  $\gamma_j$ be obtained from $\gamma_{j_1}'$ by
  (i) applying $f_j$ to timers in $\set{x_1, \ldots , x_{\beta_{j-1}}}$,
  (ii) applying $f_j$ to timers in $\set{x_1, \ldots , x_{\beta_{j-1}}}$, and
  from the beginning of $\gamma_{j-1}$ and applying $f_j$ to the remainder.
Intuitively, $\beta_0 \ldots \beta_n$ is the sequence of states
visited when $\hypoof UV$ processes $\alpha$.
\end{itemize}
%% Let $\gamma_j$ be the suffix
%% $\utlabel{i_{j+1}}{o_{j+1}}{\rho_{j+1}} \cdots \utlabel{i_n}{o_n}{\rho_n}$
%% of $\alpha$ of length $n-j$. 
By the fact that $\alpha$ is a counterexample, we have
$\beta_0\;\gamma_0 \in S \notiff  \beta_{n} \in S$, which implies that
$\beta_{j-1};\gamma_{j-1} \in S \notiff \beta_j;\gamma_j \in S$
for some $j$;
we can then take $\beta_{j-1}$ as $\beta$ and $\beta_j$ as $\beta'$, and
$\gamma$ as $\gamma_j$.
This means that $\gamma$ is a new separating suffix, and that $V$ should be
extended with the $v$ such that
$\untimedinputword(\gamma) \in \instancesof{\set{v}}$
should be added to
$V$. After adding $v$ to $V$, $U$ is no longer closed
wrt.\ $V$, so the algorithm can resume a next round of
hypothesis construction, which will eventually generate a new hypothesis,
etc.

\todobj{Should we insert a pseudocode description of the algorithm?}

The algorithm enjoys the following properties, which are similar to those
enjoyed by active automata learning algorithms for regular languages,
e.g.,~\cite{Ang87}.

\begin{theorem}
  \label{thm:alg:termination}
Given an MMT $\M$ whose normal form has $n$ states, 
algorithm XXX terminates and produces an equivalence MMT in normal form
using at most $n$ equivalence queries, and
$Y$ untimer membership queries.
\end{theorem}
   
\begin{proof}
Starting from some initial approximations (e.g., the singleton
set consisting of the empty word), the sets $U$ and $V$ are
successively extended, until $U$ contains one element of each equivalence
class of $\equiv_{S}$, and $\equiv_{S,V}$ coincides with
$\equiv_{S}$. 
At termination the hypothesis is correct, by definition of equivalence query.
During the construction, $U$ will never contain two elements that are
equivalent wrp. $\M$.
Since each round of hypothesis construction and validation adds at least one
word to $U$, there can be at most $n$ equivalence queries.
\end{proof}

\todobj{This is to be fixed}
Since each equivalence query adds only one
word to $V$, this means that $|V| \leq n$ when the algorithm finishes,
implying that in total, at most $n^2|I|$ membership queries
will be performed during hypothesis construction.
During hypothesis
validation, at most $2\log(m)$ membership queries need be performed
(in addition to the equivalence query), where
$m$ is the length of the largest counterexample word returned.

