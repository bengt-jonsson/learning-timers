\section{Algorithm for Learning of MMTs}
\label{sec:learning}

In this section, we present an algorithm for learning MMTs in then untimed
MAT of~\ref{sec:untimed-mat}, using the approximated Nerode equivalence
presented in Section~\ref{sec:approx}.
The learning algorithm follows the standard pattern for active automata learning
algorithms, such as $L^*$~\cite{Ang87}. It maintains
a set $U$ of lean behaviors, called {\em short prefixes}, which
represent states in the MMT to be constructed,
and an overapproximation of the Nerode equivalence,
parameterized by a set $V$ of input suffixes.
%% Each short prefix in $U$ is a feasible canonical untimed
%% behavior, which represents a state in the MMT to be constructed.
The learning algorithm iterates two phases: hypothesis construction and
hypothesis validation.
During hypothesis construction,
the approximation of the Nerode equivalence triggers the expansion of
$U$ and $V$ until two convergence conditions are satisfied that allow
a hypothesis automaton to be formed.
During hypothesis validation, the hypothesis automaton is submitted in an
equivalence query, and returned counterexamples are used to refine
the Nerode equivalence by expanding $V$.

\todobj{Say somewhere that we assume a fixed (un)timed language $S$.
  Make sure we make clear that it is the set of {\bf feasible} lean
behaviors of the SUL}

Let us introduce the two conditions for convergence of the construction phase.
For a lean behavior $\beta\in S$ and $\gamma \in \suffixbehs{S}{\beta}$, let
$\beta\compose{S}\gamma$ be the (unique) lean behavior $\beta'\cdot\suffmap{|\beta|}(\gamma)$ in $S$ with $\beta \sqsubseteq \beta'$.
Let $\feasibleinputs{\beta}{S}$ be the set of $i \in \extinputs$ such that
$\beta \compose{S} \simpleutlabel{i}{o} \in S$ for some $o$ (recall that the
last assignment of a lean behavior is always empty).
For $i \in \feasibleinputs{\beta}{S}$, let $\lambda(\beta,i)$ be the unique
output $o$ such that $\beta \compose{S} \simpleutlabel{i}{o} \in S$.
%% A set $V$ of generic untimed input words is {\em adequate} if it includes
%% $I \cup \set{\toevent{p}}$.
Let $U$ be a prefix-closed set of lean behaviors.
and let $V$ be an adequate set of input suffixes.
\begin{itemize}
\item
$U$ is {\em closed} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
%%   , which is either in $I$ or of form $\toevent{x_i}$ where $x_i$ is expirable after $\beta$ if
  there is a $\beta' \in U$ such that
  $\beta\compose{S}\simpleutlabel{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$.
\item
$U$ is {\em timer-consistent} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
  %% $i$, which is either in $I$ or
  %% of form $\toevent{x_i}$ for $x_i$ expirable after $\beta$,
  we have
  $\apprgetmemorable{S}{\beta\compose{S}\simpleutlabel{i}{\lambda(\beta,i)}}{V} \subseteq
  (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$.
\end{itemize}
Closedness ensures that each transition in the MMT to be constructed has a target state. 
Timer-consistency states that each timer which is needed after such
a transition (i.e., a timer set during $\beta\compose{S}\simpleutlabel{i}{\lambda(\beta,i)}$)
is either a timer active after $\beta$ or is started by the last transition, thus
getting the name $x_{(|\beta|+1)}$.
Closedness and timer-consistency allow the construction of a hypothesis MMT.

\begin{definition}[Hypothesis automaton]
\label{def:hypo}
  Let $U$ be a non-empty prefix-closed set of lean behaviors,
  and $V$ an adequate set of input suffixes such that
  $U$ is closed and timer consistent wrt.\ $V$. Then the
{\em hypothesis automaton} $\hypoof UV$ is the MMT
$\hypoof UV = (I, O, Q, q_0, \vars, \delta, \lambda, \remap)$, where
\begin{itemize}
\item $Q = U$ and $q_0 = \emptyword$,
\item $\vars$ maps each location $\beta\in U$ to $\apprgetmemorable{S}{\beta}{V}$,
\item Let $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$. Then
  \begin{itemize}
    \item $\lambda(\beta,i)$ is
     the unique $o$ such that $\extend{\beta}{i}{o} \in S$. 
    \item $\delta(\beta,i)$ is is
        the unique $\beta' \in U$ such that there is an $f$ with
  $\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V}^f \beta'$.
      \item $\remap(\beta,i):\apprgetmemorable{S}{\beta'}{V} \mapsto (\apprgetmemorable{S}{\beta}{V} \cup \natplus)$ is defined as $\circ f^{-1}$ on
        $\apprgetmemorable{S}{\beta'}{V}$ except for 
$f(x_{(|\beta|+1)})$, which is mapped to
$\apprgetassignment{S}{\beta'}{V}(f(x_{(|\beta|+1)}))$.
  \end{itemize}
\item When $\beta \in U$ and $i$ is of form $\toevent{x_j}$ in
  $\apprgetmemorable{S}{\beta}{V} \setminus \feasibleinputs{\beta}{S}$, we let
  $\lambda(\beta,i) = \bot$, 
  $\delta(\beta,i) = \beta$, and let
  $\remap(\beta,i)$ be the identity mapping on $\apprgetmemorable{S}{\beta}{V}$.
\end{itemize}
\end{definition}
The last case in the construction constructs a transition that is not feasible,
but which must anyway be present since its input is a timeout event for
 a timer which can expire after performing additional transitions from $\beta$.

 During {\bf hypothesis construction}, membership queries are performed order
 to construct the sets of form $\apprsuffixbehs{S}{\beta}{V}$ and
 $\apprsuffixbehs{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V}$ for $\beta \in U$.
 Moreoever, the sets $U$ and $V$ are expanded if needed to construct
 a hypothesis automaton.

 More precisely,
membership queries are first performed
  for all untimed input words of form $\untimedinputword(\beta) \cdot i$
   for $\beta \in U$ and $i \in \extinputs$: this allows to determine
   $\feasibleinputs{\beta}{S}$ and $\lambda(\beta,i)$
   for $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
   Thereafter, membership queries are performed
  for all untimed input words of form $\untimedinputword(\beta) \cdot v$ and
  $\untimedinputword(\beta) \cdot i \cdot v$, where
  $\beta \in U$, $v \in V$, and
  $i \in \feasibleinputs{\beta}{S}$. Note that one need only consider
  $v$ in which inputs from $\toevents$ concern timers in
  $\set{x_1, \ldots, x_{|\beta|}}$ (or in  $\set{x_1, \ldots, x_{(|\beta|+1)}}$).
  This allows to construct the sets of form $\apprsuffixbehs{S}{\beta}{V}$ and
  $\apprsuffixbehs{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}$
  for $\beta \in U$.
  It then allows to compute the approximated Nerode equivalence $\equiv_{S,V}$ on
  the set of lean behaviors of form $\beta$ and
  $\extend{\beta}{i}{\lambda(\beta,i)}$ with
  $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
  We then check whether $U$ and $V$ meet the convergence criteria.
  \begin{itemize}
    \item
  Whenever the set $U$ is not closed wrt.\ $V$, then it is extended:
if there is some $\beta \in U$ and $i$ in $\feasibleinputs{\beta}{S}$
for which there is no $\beta' \in U$ such that
$\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$, 
then $\extend{\beta}{i}{\lambda(\beta,i)}$ is added to $U$,
triggering new membership queries.
\item
  Whenever the set $U$ is not timer-consistent wrt.\ $V$, then $V$ is extended:
for timer $x_j$ in
$\apprgetmemorable{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V} \setminus (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$, find a lean
suffix behavior of form $\extend{\gamma}{\toevent{x_j}}{o'}$ in
$\apprsuffixbehs{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}$, i.e.,
a suffix such that $x_j$ expires in the last transition. This expiration is
obviously missing from $\apprsuffixbehs{S}{\beta}{V}$. But, by
adding $i\cdot\untimedinputword(\gamma)\cdot\toevent{x_j}$ to $V$, the
timer will also appear in $\apprgetmemorable{S}{\beta}{V}$.
This extension of $V$ will trigger new membership queries.
\end{itemize}
When $U$ is closed and timer-consistent wrt.\ $V$, then
a hypothesis MMT $\hypoof UV$ is constructed and
validated by submitting it in an equivalence query.
If the query returns ``yes'', then
the learning is completed, and $\hypoof UV$ accepts $S$.
If the query returns a counterexample in the form of a lean behavior
$\alpha$, this is used to extend $V$ as follows.
We assume w.l.o.g.\ that no proper prefix of $\alpha$ is a counterexample. 
By the fact that $\alpha$ is a counterexample, we can write $\alpha$ as
$(\alpha'\compose{S}\simpleutlabel{i}{o})\compose{S} \gamma$, where $\gamma$ is a lean suffix
behavior such that
$\beta\cdot \simpleutlabel{i}{o} \equiv_{S,V}^f \beta'$ but
$(\beta \compose{S} \simpleutlabel{i}{o}) \compose{S} \gamma) \in S \notiff \beta' \compose{S} f(\gamma) \in S$ for some $\beta,\beta' \in U$ such that 
$\beta\compose{S}\simpleutlabel{i}{o} \equiv_{S,V}^f \beta'$ is used to construct
the transition triggered by $i$ from $\beta$ in $\hypoof UV$.
To se this, let $\alpha = \utlabel{i_1}{o_1}{\rho_1} \cdots \utlabel{i_n}{o_n}{\rho_n}$, and for $j = 0 , \ldots, n$, define
a lean behavior $\beta_j$ and a $\beta_j$-suffix by
\begin{itemize}
\item $\beta_0 = \emptyword$, and
%%   , $f_0$ is arbitrary, and 
  $\gamma_0$ is obtained from $\alpha$ by renaming each timer $x_l$ to $y_l$
\item
  Let $\gamma_{j-1}$ be of form
  $\simpleutlabel{\hat{i_j}}{\hat{o_j}} \gamma_{j-1}'$,
  let $\beta_j$ be $\lambda(\beta_{j-1},\hat{i_j})$,
  let $f_j$ be the mapping used to establish
  $\beta_{j-1}\compose{S}\simpleutlabel{\hat{i_j}}{\hat{o_j}} \equiv_{S,V}^{f_j} \beta_{j}$
  in the construction of $\hypoof UV$,
  and let $\gamma_j$ be obtained from $\gamma_{j-1}'$ by
  (i) applying $f_j$ to timers in $\set{x_1, \ldots , x_{\beta_{j-1}}}$,
  (ii) replacing $y_1$ by $f_j(x_{\beta_{j-1}+1})$, and
  (ii) replacing each $y_l$ by $y_{l-1}$.
\end{itemize}
Intuitively, the resuls is that
$\beta_0 \ldots \beta_n$ is the sequence of states 
visited when $\hypoof UV$ processes $\alpha$, and
$\gamma_j$ is the $\beta_j$-suffix which corresponds to a suffix of $\alpha$.
%% Let $\gamma_j$ be the suffix
%% $\utlabel{i_{j+1}}{o_{j+1}}{\rho_{j+1}} \cdots \utlabel{i_n}{o_n}{\rho_n}$
%% of $\alpha$ of length $n-j$. 
By the fact that $\alpha$ is a counterexample, we have
$\beta_0\compose{S}\gamma_0 \in S \notiff  \beta_{n} \in S$, which implies that
$\beta_{j-1}\compose{S}\gamma_{j-1} \in S \notiff \beta_j\compose{S}\gamma_j \in S$
for some $j$;
we can then take $\beta_{j-1}$ as $\beta$ and $\beta_j$ as $\beta'$, and
$\gamma$ as $\gamma_j$.
This means that $\gamma$ is a new separating suffix, and that $V$ should be
extended with $\untimedinputword(\gamma)$.
After adding $v$ (and its permutations) to $V$, $U$ is no longer closed
wrt.\ $V$, so the algorithm can resume a next round of
hypothesis construction, which will eventually generate a new hypothesis, etc.

Let us analyze the number of queries that may be required in the worst case
to learn an MMT whose canonical form has $n$ states and at most $r$ active
timers in any state.
\begin{itemize}
\item
  Each round of hypothesis construction may need $n\cdot(|I|+r+1)-\cdot |V|^r$
  membership queries. The factor $|V|^r$ arises from the possible
  need to consider all   permutations of prefix-timers in each suffix.
\item
  The processing of each counterexample can be done using $\log(m)$ queries,
  using binary search, where $m$ is the length of the counterexample.
\item
  Each equivalence query will result in refuting an equivalence of form
  $\beta\cdot \simpleutlabel{i}{o} \equiv_{S,V}^f \beta'$, and
  adding an additional suffix to $V$. There are at most
  $r !$ possible permutations $f$ for each $\beta\cdot \simpleutlabel{i}{o}$,
  implying that there can be at most $n \cdot r!$ equivalence queries.
\end{itemize}
In summary,
the algorithm enjoys the following properties, which are analogous
to properties
enjoyed by active automata learning algorithms for regular languages,
e.g.,~\cite{Ang87}. The additional complexity caused by timers is analogous
to the additional complexity caused by registers in learning of register
automata~\cite{HowarSJC12,CasselHJS16}.

\begin{theorem}
  \label{thm:alg:termination}
  Given an MMT $\M$ whose normal form has $n$ states, the active learning
  procedure of this section terminates and produces an equivalen MMT in
  canonical form, using at most $n\cdot r!$ equivalence queries, and
  \todobj{Should we put the exact bound here?} membership queries.
\end{theorem}


\todobj{This is to be fixed, or remove the rest of this}

Starting from some initial approximations (e.g., the singleton
set consisting of the empty word), the sets $U$ and $V$ are
successively extended, until $U$ contains one element of each equivalence
class of $\equiv_{S}$, and $\equiv_{S,V}$ coincides with
$\equiv_{S}$. 
At termination the hypothesis is correct, by definition of equivalence query.
During the construction, $U$ will never contain two elements that are
equivalent wrp. $\M$.
Since each round of hypothesis construction and validation adds at least one
word to $U$, there can be at most $n$ equivalence queries.


Since each equivalence query adds only one
word to $V$, this means that $|V| \leq n$ when the algorithm finishes,
implying that in total, at most $n^2|I|$ membership queries
will be performed during hypothesis construction.
During hypothesis
validation, at most $2\log(m)$ membership queries need be performed
(in addition to the equivalence query), where
$m$ is the length of the largest counterexample word returned.

