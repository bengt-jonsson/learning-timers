\section{Algorithm for Learning of MMTs}
\label{sec:learning}

In this section, we present an algorithm for learning MMTs in then untimed
MAT of~\ref{sec:untimed-mat}, using the approximated Nerode equivalence
presented in Section~\ref{sec:approx}.
The learning algorithm follows the standard pattern for active automata learning
algorithms, such as $L^*$~\cite{Ang87}. It maintains
a set $U$ of lean behaviors, called {\em short prefixes}, which
represent states in the MMT to be constructed,
and an overapproximation of the Nerode equivalence,
parameterized by a set $V$ of input suffixes.
%% Each short prefix in $U$ is a feasible canonical untimed
%% behavior, which represents a state in the MMT to be constructed.
The learning algorithm iterates two phases: hypothesis construction and
hypothesis validation.
During hypothesis construction,
the approximation of the Nerode equivalence triggers the expansion of
$U$ and $V$ until two convergence conditions are satisfied that allow
a hypothesis automaton to be formed.
During hypothesis validation, the hypothesis automaton is submitted in an
equivalence query, and returned counterexamples are used to refine
the Nerode equivalence by expanding $V$.

\todobj{Say somewhere that we assume a fixed (un)timed language $S$.
  Make sure we make clear that it is the set of {\bf feasible} lean
behaviors of the SUL}

Let us introduce the two conditions for convergence of the construction phase.
For a feasible lean behavior $\beta$, let $\feasibleinputs{\beta}{S}$ be the
set of $i \in \extinputs$ such that
$\beta ; \simpleutlabel{i}{o} \in S$ for some $o$ (recall that the
last assignment of a lean behavior is always empty).
For $i \in \feasibleinputs{\beta}{S}$, let $\lambda(\beta,i)$ be the unique
output $o$ such that $\beta ; \simpleutlabel{i}{o} \in S$.
%% A set $V$ of generic untimed input words is {\em adequate} if it includes
%% $I \cup \set{\toevent{p}}$.
Let $U$ be a prefix-closed set of lean behaviors.
and let $V$ be an adequate set of input suffixes.
\begin{itemize}
\item
$U$ is {\em closed} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
%%   , which is either in $I$ or of form $\toevent{x_i}$ where $x_i$ is expirable after $\beta$ if
  there is a $\beta' \in U$ such that
  $\beta;\simpleutlabel{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$.
\item
$U$ is {\em timer-consistent} wrt.\ $V$ if 
  for each $\beta \in U$ and
  $i \in \feasibleinputs{\beta}{S}$
  %% $i$, which is either in $I$ or
  %% of form $\toevent{x_i}$ for $x_i$ expirable after $\beta$,
  we have
  $\apprgetmemorable{S}{\beta;\simpleutlabel{i}{\lambda(\beta,i)}}{V} \subseteq
  (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$.
\end{itemize}
Closedness ensures that each transition in the MMT to be constructed has a target state. 
Timer-consistency states that each timer which is needed after such
a transition (i.e., a timer set during $\beta;\simpleutlabel{i}{\lambda(\beta,i)}$)
is either a timer active after $\beta$ or is started by the last transition, thus
getting the name $x_{(|\beta|+1)}$.
Closedness and timer-consistency allow the construction of a hypothesis MMT.

\begin{definition}[Hypothesis automaton]
\label{def:hypo}
  Let $U$ be a non-empty prefix-closed set of lean behaviors,
  and $V$ an adequate set of input suffixes such that
  $U$ is closed and timer consistent wrt.\ $V$. Then the
{\em hypothesis automaton} $\hypoof UV$ is the MMT
$\hypoof UV = (I, O, Q, q_0, \vars, \delta, \lambda, \remap)$, where
\begin{itemize}
\item $Q = U$ and $q_0 = \emptyword$,
\item $\vars$ maps each location $\beta\in U$ to $\apprgetmemorable{S}{\beta}{V}$,
\item Let $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$. Then
  \begin{itemize}
    \item $\lambda(\beta,i)$ is
     the unique $o$ such that $\extend{\beta}{i}{o} \in S$. 
    \item $\delta(\beta,i)$ is is
        the unique $\beta' \in U$ such that there is an $f$ with
  $\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V}^f \beta'$.
  \item $\remap(\beta,i):\apprgetmemorable{S}{\beta'}{V} \mapsto (\apprgetmemorable{S}{\beta}{V} \cup \natplus)$ is defined as $\rho \circ f^{-1}$, where $\rho$ maps timer $x_{(|\beta|+1)}$ to
$\apprgetassignment{S}{(\extend{\beta}{i}{\lambda(\beta,i)})}{V}(x_{(|\beta|+1)})$.
  \end{itemize}
\item When $\beta \in U$ and $i$ is of form $\toevent{x_j}$ in
  $\apprgetmemorable{S}{\beta}{V} \setminus \feasibleinputs{\beta}{S}$, we let
  $\lambda(\beta,i) = \bot$, 
  $\delta(\beta,i) = \beta$, and let
  $\remap(\beta,i)$ be the identity mapping on $\apprgetmemorable{S}{\beta}{V}$.
\end{itemize}
\end{definition}
The last case in the construction constructs a transition that is not feasible,
but which must anyway be present since its input is a timeout event for
 a timer which can expire in some other continuation of $\beta$.

During {\bf hypothesis construction}, membership queries are performed:
\begin{itemize}
\item for all untimed input words of form $\untimedinputword(\beta) \cdot i$
   for $\beta \in U$ and $i \in \extinputs$: this allows to determine
   $\feasibleinputs{\beta}{S}$ and $\lambda(\beta,i)$
   for $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
\item
  for all untimed input words of form $\untimedinputword(\beta) \cdot v$ and
  $\untimedinputword(\beta) \cdot i \cdot v$, where
  $\beta \in U$, $v \in \instancesof{V}$, and
  $i \in \feasibleinputs{\beta}{S}$.
    This allows to compute the approximated Nerode equivalence $\equiv_{S,V}$ on
    the set of untimed words of form $\beta$ and
    $\extend{\beta}{i}{\lambda(\beta,i)}$ with
    $\beta \in U$ and $i \in \feasibleinputs{\beta}{S}$.
    Whenever the set $U$ is not closed wrt.\ $V$, then it is extended:
if there is some $\beta \in U$ and $i$ in $\feasibleinputs{\beta}{S}$
for which there is no $o$ and $\beta' \in U$ such that
$\extend{\beta}{i}{\lambda(\beta,i)} \equiv_{S,V} \beta'$, 
then $\extend{\beta}{i}{\lambda(\beta,i)}$ is added to $U$,
triggering new membership queries.
Whenever the set $U$ is not timer-consistent wrt.\ $V$, then $V$ is extended:
for timer $x_j$ in
$\apprgetmemorable{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V} \setminus (\apprgetmemorable{S}{\beta}{V} \cup \set{x_{(|\beta|+1)}})$, find an 
untimed behavior of form $\extend{\gamma}{\toevent{x_i}}{o'}$ in
$\apprsuffixbehs{S}{\extend{\beta}{i}{\lambda(\beta,i)}}{V}$.
Then add $i\cdot\untimedinputword(\gamma)\cdot\toevent{p}$ to $V$.
\end{itemize}

When $U$ is closed and timer-consistent wrt.\ $V$, then
a hypothesis MMT $\hypoof UV$ is constructed and
validated by submitting it in an equivalence query.
If the query returns ``yes'', then
the learning is completed, and $\hypoof UV$ accepts $S$.
If the query returns a counterexample word $\alpha$, this is used to extend
$V$ as follows. We assume w.l.o.g.\ that no proper prefix of $\alpha$ is
a counterexample. 
By the fact that $\alpha$ is a counterexample, there is a suffix
$i/o,\rho \cdot \gamma$ of $\alpha$
such that $\beta\cdot i/o,\rho \equiv_{S,V} \beta'$ but
$\beta \cdot i/o,\rho \cdot \gamma \in S \notiff \beta' \cdot \gamma \in S$
for some $\beta,\beta' \in U$.
(To se this, let $\alpha = i_1/o_1,\rho_1 \cdots i_n/o_n,\rho_n$, and
define the sequence $\beta_0, \beta_1, \ldots ,\beta_{n}$ of short prefixes
in $U$ by $\beta_0 = \emptyword$ and
$\beta_{j-1}i_j/o_j,\rho_j \equiv_{S,V} \beta_{i}$ for
$j = 1, \ldots n$, i.e., $\beta_0 \ldots \beta_n$ is the sequence of states
visited when $\hypoof UV$ processes $\alpha$.
Let $\gamma_j$ be the suffix $i_{j+1}/o_{j+1}\rho_{j+1} \cdots i_n/o_n,\rho_n$
of $\alpha$ of length $n-j$. 
By the fact that $\alpha$ is a counterexample, we have
$\beta_0\cdot\gamma_0 \in S \notiff  \beta_{n} \in S$, which implies that
$\beta_{j-1}\cdot\gamma_{j-1} \in S \notiff \beta_j\cdot\gamma_j \in S$
for some $j$;
we can then take $\beta_{j-1}$ as $\beta$ and $\beta_j$ as $\beta'$.)
This means that $\gamma$ is a new separating suffix, and that $V$ should be
extended with the $v$ such that
$\untimedinputword(\gamma) \in \instancesof{\set{v}}$
should be added to
$V$. After adding $v$ to $V$, $U$ is no longer closed
wrt.\ $V$, so the algorithm can resume a next round of
hypothesis construction, which will eventually generate a new hypothesis,
etc.

\todobj{Should we insert a pseudocode description of the algorithm?}

The algorithm enjoys the following properties, which are similar to those
enjoyed by active automata learning algorithms for regular languages,
e.g.,~\cite{Ang87}.

\begin{theorem}
  \label{thm:alg:termination}
Given an MMT $\M$ whose normal form has $n$ states, 
algorithm XXX terminates and produces an equivalence MMT in normal form
using at most $n$ equivalence queries, and
$Y$ untimer membership queries.
\end{theorem}
   
\begin{proof}
Starting from some initial approximations (e.g., the singleton
set consisting of the empty word), the sets $U$ and $V$ are
successively extended, until $U$ contains one element of each equivalence
class of $\equiv_{S}$, and $\equiv_{S,V}$ coincides with
$\equiv_{S}$. 
At termination the hypothesis is correct, by definition of equivalence query.
During the construction, $U$ will never contain two elements that are
equivalent wrp. $\M$.
Since each round of hypothesis construction and validation adds at least one
word to $U$, there can be at most $n$ equivalence queries.
\end{proof}

\todobj{This is to be fixed}
Since each equivalence query adds only one
word to $V$, this means that $|V| \leq n$ when the algorithm finishes,
implying that in total, at most $n^2|I|$ membership queries
will be performed during hypothesis construction.
During hypothesis
validation, at most $2\log(m)$ membership queries need be performed
(in addition to the equivalence query), where
$m$ is the length of the largest counterexample word returned.

